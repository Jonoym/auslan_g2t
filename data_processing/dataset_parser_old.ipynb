{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pympi\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(token):    \n",
    "    if token[:8] == \"FBUOY:DS\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 2:\n",
    "            token = token.split(':')[2]\n",
    "    if token[:6] == \"FBUOY:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    if token[:6] == \"FUBOY:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    if token[:6] == \"FBOUY:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    if token[:6] == \"GICA):\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    if token[:6] == \"TBUOY:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:3] == \"FS:\":\n",
    "        token = token[3:]\n",
    "    elif token[:3] == \"FB:\":\n",
    "        token = token[3:]\n",
    "    elif token[:2] == \"DS\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:2] == \"G(\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:3] == \"CA:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:3] == \"GA:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:2] == \"G:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:2] == \"M:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "\n",
    "    index = token.find(\"-2H\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "    index = token.find(\"-1H\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "    index = token.find(\"2-H\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "    index = token.find(\"1-H\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "\n",
    "    index = token.find(\"(\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory, dataset_name):\n",
    "    \n",
    "    data_file = open(dataset_name, 'w')\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if '.eaf' in filename:\n",
    "            append_dataset(directory, filename, data_file)\n",
    "            \n",
    "    data_file.close()\n",
    "\n",
    "def append_dataset(directory, eaf_file, data_file):\n",
    "    eaf_object = pympi.Elan.Eaf(f'./{directory}/{eaf_file}')\n",
    "    \n",
    "    if not has_translation(eaf_object):\n",
    "        return\n",
    "    \n",
    "    print(eaf_file)\n",
    "    \n",
    "    sentence, tokens = parse_eaf(eaf_object)\n",
    "    \n",
    "    tokens = filter_tokens(tokens)\n",
    "    for i in range(0, len(sentence)):\n",
    "        if len(tokens[i]) - len(sentence[i]) > 25:\n",
    "            print(f'{sentence[i]}\\t{tokens[i]}\\n')\n",
    "            continue\n",
    "        data_file.write(f'{sentence[i]}\\t{tokens[i]}\\n')\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token_sequence in tokens:\n",
    "        filtered_token_sequence = []\n",
    "        \n",
    "        for token in token_sequence:\n",
    "            \n",
    "            if token[:3] == \"PT:\":\n",
    "                continue\n",
    "            if token[:5] == \"LOOK(\":\n",
    "                continue\n",
    "            if token[:7] == \"PTBUOY:\":\n",
    "                continue\n",
    "            \n",
    "            token = get_token(token)\n",
    "                    \n",
    "            if token in [\"WELL\", \"\", \"AHH\", \"UMM\", \"FSL\", \"PTBUOY\"]:\n",
    "                continue\n",
    "            \n",
    "            if token[-1].isdigit():\n",
    "                token = token[:-1]\n",
    "                \n",
    "            if len(token) < 2:\n",
    "                continue\n",
    "            \n",
    "            filtered_token_sequence.append(token)\n",
    "        filtered_tokens.append(filtered_token_sequence)\n",
    "                \n",
    "    unduped_tokens = []\n",
    "    for token_group in filtered_tokens:\n",
    "        filtered_token_group = []\n",
    "        for i in range(0, len(token_group)):\n",
    "            if (i == 0 or token_group[i] != token_group[i - 1]) and (i < 2 or token_group[i] != token_group[i - 2]):\n",
    "                filtered_token_group.append(token_group[i])\n",
    "            \n",
    "        unduped_tokens.append(join_tokens(filtered_token_group))\n",
    "        \n",
    "    return unduped_tokens\n",
    "\n",
    "def has_translation(eaf_object):\n",
    "    return eaf_object.tiers.get('FreeTransl') is not None\n",
    "\n",
    "def parse_eaf(eaf_object):\n",
    "    filtered_sentences = filter_tier(eaf_object, 'FreeTransl')\n",
    "    filtered_lh = filter_tier(eaf_object, 'LH-IDgloss')\n",
    "    filtered_rh = filter_tier(eaf_object, 'RH-IDgloss')\n",
    "    \n",
    "    lhp = 0\n",
    "    rhp = 0\n",
    "    tokens = []\n",
    "    for sentence in filtered_sentences:\n",
    "        token_group = []\n",
    "        while (lhp < len(filtered_lh) and filtered_lh[lhp][1] <= sentence[1] + 2):\n",
    "            token_group.append(filtered_lh[lhp])\n",
    "            lhp += 1\n",
    "        \n",
    "        while (rhp < len(filtered_rh) and filtered_rh[rhp][1] <= sentence[1] + 2):\n",
    "            token_group.append(filtered_rh[rhp])\n",
    "            rhp += 1\n",
    "        tokens.append(token_group)\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token_group in tokens:\n",
    "        token_group.sort()\n",
    "        filtered_token_group = []\n",
    "        for i in range(0, len(token_group)):\n",
    "            if i == 0 or token_group[i][2] != token_group[i - 1][2]:\n",
    "                filtered_token_group.append(token_group[i][2])\n",
    "            \n",
    "        filtered_tokens.append(filtered_token_group)\n",
    "    \n",
    "    sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        sentences.append(sentence[2])\n",
    "        \n",
    "    return sentences, filtered_tokens\n",
    "\n",
    "def filter_tier(eaf_object, tier_name):\n",
    "    segment_dict = eaf_object.tiers[tier_name][0]\n",
    "    \n",
    "    filtered_segments = []\n",
    "    \n",
    "    for key in segment_dict:\n",
    "        segment = segment_dict[key]\n",
    "        if segment[2] != '':\n",
    "            filtered_segments.append((int(segment[0][2:]), int(segment[1][2:]), segment[2]))\n",
    "        \n",
    "    return filtered_segments\n",
    "                        \n",
    "def join_tokens(tokens):\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce64e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'EAF'\n",
    "dataset_name = 'elan_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(directory, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eaf_object = pympi.Elan.Eaf(f'./{directory}/AAPB1c2b.eaf')\n",
    "filtered_sentences = filter_tier(eaf_object, 'RH-IDgloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa90652",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82b4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f732ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
