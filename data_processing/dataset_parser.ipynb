{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pympi\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca09ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory, dataset_name):\n",
    "    \n",
    "    data_file = open(dataset_name, 'w')\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if '.eaf' in filename:\n",
    "            append_dataset(directory, filename, data_file)\n",
    "            \n",
    "    data_file.close()\n",
    "    \n",
    "def append_dataset(directory, eaf_file, data_file):\n",
    "    eaf_object = pympi.Elan.Eaf(f'./{directory}/{eaf_file}')\n",
    "    \n",
    "    if not has_translation(eaf_object):\n",
    "        return\n",
    "        \n",
    "    sentence, tokens = parse_eaf(eaf_object)\n",
    "    \n",
    "    tokens = filter_tokens(tokens)\n",
    "    for i in range(0, len(sentence)):\n",
    "        if len(tokens[i]) - len(sentence[i]) > 25:\n",
    "            print(f'{sentence[i]}\\t{tokens[i]}\\n')\n",
    "            continue\n",
    "        data_file.write(f'{sentence[i]}\\t{join_tokens(tokens[i])}\\n')\n",
    "    \n",
    "def filter_tokens(tokens):\n",
    "    return tokens\n",
    "    \n",
    "def has_translation(eaf_object):\n",
    "    return eaf_object.tiers.get('FreeTransl') is not None\n",
    "\n",
    "lit_transls = []\n",
    "test = []\n",
    "\n",
    "def parse_eaf(eaf_object):\n",
    "    filtered_sentences = filter_tier(eaf_object, 'FreeTransl')\n",
    "    test.append(filtered_sentences)\n",
    "    lit_translation = filter_tier(eaf_object, 'LitTransl')\n",
    "    lit_transls.append(lit_translation)\n",
    "    filtered_lh = filter_tier(eaf_object, 'LH-IDgloss')\n",
    "    filtered_rh = filter_tier(eaf_object, 'RH-IDgloss')\n",
    "    \n",
    "    lhp = 0\n",
    "    rhp = 0\n",
    "    tokens = []\n",
    "    for sentence in filtered_sentences:\n",
    "        token_group = []\n",
    "        while (lhp < len(filtered_lh) and filtered_lh[lhp][1] <= sentence[1] + 2):\n",
    "            token_group.append(filtered_lh[lhp])\n",
    "            lhp += 1\n",
    "        \n",
    "        while (rhp < len(filtered_rh) and filtered_rh[rhp][1] <= sentence[1] + 2):\n",
    "            token_group.append(filtered_rh[rhp])\n",
    "            rhp += 1\n",
    "        tokens.append(token_group)\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token_group in tokens:\n",
    "        token_group.sort()\n",
    "        filtered_token_group = []\n",
    "        for i in range(0, len(token_group)):\n",
    "            if i == 0 or token_group[i][2] != token_group[i - 1][2]:\n",
    "                filtered_token_group.append(token_group[i][2])\n",
    "            \n",
    "        filtered_tokens.append(filtered_token_group)\n",
    "    \n",
    "    sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        sentences.append(sentence[2])\n",
    "        \n",
    "    return sentences, filtered_tokens\n",
    "\n",
    "def filter_tier(eaf_object, tier_name):\n",
    "    segment_dict = eaf_object.tiers[tier_name][0]\n",
    "    \n",
    "    filtered_segments = []\n",
    "    \n",
    "    for key in segment_dict:\n",
    "        segment = segment_dict[key]\n",
    "        if segment[2] != '':\n",
    "            filtered_segments.append((int(segment[0][2:]), int(segment[1][2:]), segment[2]))\n",
    "        \n",
    "    return filtered_segments\n",
    "                        \n",
    "def join_tokens(tokens):\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'EAF'\n",
    "output_filename = 'elan_dataset_testing.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(directory, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de05204",
   "metadata": {},
   "outputs": [],
   "source": [
    "eaf_object = pympi.Elan.Eaf(f'./{directory}/AAPB1c2b.eaf')\n",
    "filtered_sentences = filter_tier(eaf_object, 'RH-IDgloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf37854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70b2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832645f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd574f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff909e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lit_transl_count = []\n",
    "free_transl_count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09301574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory, dataset_name):\n",
    "    \n",
    "    data_file = open(dataset_name, 'w')\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if '.eaf' in filename:\n",
    "            append_dataset(directory, filename, data_file)\n",
    "            \n",
    "    data_file.close()\n",
    "    \n",
    "def append_dataset(directory, eaf_file, data_file):\n",
    "    eaf_object = pympi.Elan.Eaf(f'./{directory}/{eaf_file}')\n",
    "    \n",
    "    if not has_translation(eaf_object, 'LitTransl'):\n",
    "        return\n",
    "        \n",
    "    sentence, tokens = parse_eaf(eaf_object)\n",
    "    \n",
    "    tokens = filter_tokens(tokens)\n",
    "    for i in range(0, len(sentence)):\n",
    "#         if len(tokens[i]) - len(sentence[i]) > 25:\n",
    "#             print(f'{sentence[i]}\\t{tokens[i]}\\n')\n",
    "#             continue\n",
    "        data_file.write(f'{sentence[i]}\\t{join_tokens(tokens[i])}\\n')\n",
    "    \n",
    "def filter_tokens(tokens):\n",
    "    return tokens\n",
    "    \n",
    "def has_translation(eaf_object, translation_type):\n",
    "    return eaf_object.tiers.get(translation_type) is not None\n",
    "\n",
    "def parse_eaf(eaf_object):\n",
    "    filtered_sentences = filter_tier(eaf_object, 'MouthGestF')\n",
    "    lit_translation = filter_tier(eaf_object, 'LitTransl')\n",
    "    filtered_lh = filter_tier(eaf_object, 'LH-IDgloss')\n",
    "    filtered_rh = filter_tier(eaf_object, 'RH-IDgloss')\n",
    "    \n",
    "    \n",
    "    \n",
    "    lhp = 0\n",
    "    rhp = 0\n",
    "    tokens = []\n",
    "    for sentence in filtered_sentences:\n",
    "        token_group = []\n",
    "        \n",
    "        while (lhp < len(filtered_lh) and filtered_lh[lhp][1] <= sentence[1] + 2 and rhp < len(filtered_rh) and filtered_rh[rhp][1] <= sentence[1] + 2):\n",
    "            if (filtered_lh[lhp][0] < filtered_rh[rhp][0]):\n",
    "                token_group.append(filtered_lh[lhp])\n",
    "#                 print(filtered_lh[lhp])\n",
    "                lhp += 1\n",
    "            else:\n",
    "                token_group.append(filtered_rh[rhp])\n",
    "#                 print(filtered_rh[rhp])\n",
    "                rhp += 1\n",
    "        while (lhp < len(filtered_lh) and filtered_lh[lhp][1] <= sentence[1] + 2):\n",
    "            token_group.append(filtered_lh[lhp])\n",
    "#             print(filtered_lh[lhp])\n",
    "            lhp += 1\n",
    "        \n",
    "        while (rhp < len(filtered_rh) and filtered_rh[rhp][1] <= sentence[1] + 2):\n",
    "            token_group.append(filtered_rh[rhp])\n",
    "#             print(filtered_rh[rhp])\n",
    "            rhp += 1\n",
    "\n",
    "        tokens.append(token_group)\n",
    "        \n",
    "        just_tokens = []\n",
    "        for x in token_group:\n",
    "            just_tokens.append(x)\n",
    "            \n",
    "#         print(just_tokens)\n",
    "        \n",
    "#     print(tokens)\n",
    "#     print()\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token_group in tokens:\n",
    "        token_group.sort()\n",
    "        for x in token_group:\n",
    "            print(x)\n",
    "        filtered_token_group = []\n",
    "        for i in range(0, len(token_group)):\n",
    "            if (i == 0 or token_group[i][2] != token_group[i - 1][2]) and (i < 2 or token_group[i][2] != token_group[i - 2][2]):\n",
    "                filtered_token_group.append(token_group[i][2])\n",
    "            \n",
    "        filtered_tokens.append(filtered_token_group)\n",
    "#         for x in filtered_token_group:\n",
    "#             print(x)\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "    \n",
    "    sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        sentences.append(sentence[2])\n",
    "        \n",
    "    return sentences, filtered_tokens\n",
    "\n",
    "def filter_tier(eaf_object, tier_name):\n",
    "    segment_dict = eaf_object.tiers[tier_name][0]\n",
    "    \n",
    "    filtered_segments = []\n",
    "    \n",
    "    for key in segment_dict:\n",
    "        segment = segment_dict[key]\n",
    "        if segment[2] != '':\n",
    "            filtered_segments.append((int(segment[0][2:]), int(segment[1][2:]), segment[2]))\n",
    "        \n",
    "    return filtered_segments\n",
    "                        \n",
    "def join_tokens(tokens):\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55759b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'EAF'\n",
    "output_filename = 'null.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(directory, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35535f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lit_transl_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(free_transl_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b970f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0a0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2035f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57152f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d218ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084c40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory, dataset_name):\n",
    "    \n",
    "    data_file = open(dataset_name, 'w')\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if '.eaf' in filename:\n",
    "            append_dataset(directory, filename, data_file)\n",
    "            \n",
    "    data_file.close()\n",
    "    \n",
    "def append_dataset(directory, eaf_file, data_file):\n",
    "    eaf_object = pympi.Elan.Eaf(f'./{directory}/{eaf_file}')\n",
    "    \n",
    "    if not has_translation(eaf_object, 'FreeTransl') or not has_translation(eaf_object, 'LitTransl'):\n",
    "        print(\"no translation\")\n",
    "        return\n",
    "        \n",
    "    sentence, tokens = parse_eaf(eaf_object)\n",
    "    \n",
    "    tokens = filter_tokens(tokens)\n",
    "    for i in range(0, len(sentence)):\n",
    "        if len(tokens[i]) - len(sentence[i]) > 25:\n",
    "            continue\n",
    "        data_file.write(f'{sentence[i]}\\t{join_tokens(tokens[i])}\\n')\n",
    "    \n",
    "def filter_tokens(tokens):\n",
    "    return tokens\n",
    "    \n",
    "def has_translation(eaf_object, translation):\n",
    "    return eaf_object.tiers.get(translation) is not None\n",
    "\n",
    "def parse_eaf(eaf_object):\n",
    "    free_filtered_sentences = filter_tier(eaf_object, 'FreeTransl')\n",
    "    lit_filtered_sentences = filter_tier(eaf_object, 'LitTransl')\n",
    "    filtered_lh = filter_tier(eaf_object, 'LH-IDgloss')\n",
    "    filtered_rh = filter_tier(eaf_object, 'RH-IDgloss')\n",
    "    \n",
    "    if (len(free_filtered_sentences) == 0 and len(lit_filtered_sentences) == 0):\n",
    "        print(\"hello\")\n",
    "        filtered_sentences = free_filtered_sentences\n",
    "    elif len(free_filtered_sentences) != 0:\n",
    "        filtered_sentences = free_filtered_sentences\n",
    "    elif len(lit_filtered_sentences) != 0:\n",
    "        filtered_sentences = lit_filtered_sentences\n",
    "    \n",
    "    if len(free_filtered_sentences) != 0 and len(lit_filtered_sentences) != 0:\n",
    "        print(\"BOTH TRANSLATIONS\")\n",
    "        filtered_sentences = lit_filtered_sentences\n",
    "\n",
    "        \n",
    "        \n",
    "    lhp = 0\n",
    "    rhp = 0\n",
    "    tokens = []\n",
    "    for sentence in filtered_sentences:\n",
    "        token_group = []\n",
    "        while (lhp < len(filtered_lh) and filtered_lh[lhp][1] <= sentence[1] + 2):\n",
    "            token_group.append(filtered_lh[lhp])\n",
    "            lhp += 1\n",
    "        \n",
    "        while (rhp < len(filtered_rh) and filtered_rh[rhp][1] <= sentence[1] + 2):\n",
    "            token_group.append(filtered_rh[rhp])\n",
    "            rhp += 1\n",
    "        tokens.append(token_group)\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for token_group in tokens:\n",
    "        token_group.sort()\n",
    "        filtered_token_group = []\n",
    "        for i in range(0, len(token_group)):\n",
    "            if i == 0 or token_group[i][2] != token_group[i - 1][2]:\n",
    "                filtered_token_group.append(token_group[i][2])\n",
    "            \n",
    "        filtered_tokens.append(filtered_token_group)\n",
    "    \n",
    "    sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        sentences.append(sentence[2])\n",
    "        \n",
    "    return sentences, filtered_tokens\n",
    "\n",
    "def filter_tier(eaf_object, tier_name):\n",
    "    segment_dict = eaf_object.tiers[tier_name][0]\n",
    "    \n",
    "    filtered_segments = []\n",
    "    \n",
    "    for key in segment_dict:\n",
    "        segment = segment_dict[key]\n",
    "        if segment[2] != '':\n",
    "            filtered_segments.append((int(segment[0][2:]), int(segment[1][2:]), segment[2]))\n",
    "        \n",
    "    return filtered_segments\n",
    "                        \n",
    "def join_tokens(tokens):\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa80d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'EAF'\n",
    "output_filename = 'elan_dataset_merged.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(directory, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c0a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04641bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1444d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
