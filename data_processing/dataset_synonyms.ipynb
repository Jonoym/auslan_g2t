{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38353211",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"elan_cleaned_nodup_spellcheck.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename, \"r\")\n",
    "lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19116305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(token):\n",
    "\n",
    "    # Specify the URL you want to fetch data from\n",
    "    url = f'https://auslan.org.au/dictionary/words/{token}-1.html'\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Print the content of the response (HTML, JSON, etc.)\n",
    "        start = response.text.find(\"<strong>Keywords:</strong>\")\n",
    "        \n",
    "        end = (response.text[start:]).find(\"</p>\")\n",
    "        keywords = response.text[start: start + end]\n",
    "        keywords = keywords.replace(\"&#x27;\",\"'\")\n",
    "        keywords = keywords.replace(\"<b>\",\"\")\n",
    "        keywords = keywords.replace(\"</b>\",\"\")\n",
    "        keywords = keywords.replace(\" \",\"\")\n",
    "        keywords = keywords.replace(\",\",\"\")\n",
    "        keywords = keywords.replace(\"&#x27;\",\"'\")\n",
    "\n",
    "        keywords = keywords.strip()\n",
    "        keywords = keywords.split(\"\\n\")\n",
    "        count = 0\n",
    "        filtered_keywords = []\n",
    "        for keyword in keywords:\n",
    "            if count == 0:\n",
    "                count += 1\n",
    "                continue\n",
    "            if (keyword == \"\"):\n",
    "                continue\n",
    "            \n",
    "            brackets = keyword.find(\"(\")\n",
    "            \n",
    "            if (brackets != -1):\n",
    "                filtered_keywords.append(keyword[:brackets])\n",
    "            else:\n",
    "                filtered_keywords.append(keyword)\n",
    "            \n",
    "        \n",
    "        return filtered_keywords\n",
    "        \n",
    "    else:\n",
    "        return [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dict = {}\n",
    "token_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "        \n",
    "    for word in parts[0].split(\" \"):\n",
    "        word = word.strip('.,!:;\"').lower()\n",
    "        if word in sentence_dict:\n",
    "            sentence_dict[word] += 1\n",
    "        else:\n",
    "            sentence_dict[word] = 1\n",
    "        \n",
    "    for token in parts[1].split(\" \"):\n",
    "        token = token.strip('.,!:;\"\\n').upper()\n",
    "        if token in token_dict:\n",
    "            token_dict[token] += 1\n",
    "        else:\n",
    "            token_dict[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149d102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentence_dict = sorted(sentence_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_token_dict = sorted(token_dict.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a69632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonyms = {}\n",
    "# for token in token_dict.keys():\n",
    "#     synonyms[token] = fetch(token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246cce65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04175223",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, freq in sorted_token_dict:\n",
    "    for alt in synonyms[token]:\n",
    "        if alt.upper() in token_dict.keys() and alt.upper() != token:\n",
    "            if  token_dict[alt.upper()] > 20:\n",
    "                print(token)\n",
    "                print(token_dict[token])\n",
    "                print(alt.upper())\n",
    "                print(token_dict[alt.upper()])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_lines = []\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "    sentence = parts[0]\n",
    "    sequence = parts[1]\n",
    "    \n",
    "    new_sentence = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        word = word.strip('\\'.,!:;\"\\n').lower()\n",
    "        new_sentence.append(word)\n",
    "        \n",
    "    new_sequence = []\n",
    "    for token in sequence.split(\" \"):\n",
    "        token = token.strip('\\'.,!:;\"\\n').upper()\n",
    "        if token == \"INDETERMINATE\":\n",
    "            continue\n",
    "        most_common_freq = token_dict[token]\n",
    "        most_common = token\n",
    "        for alt in synonyms[token]:\n",
    "            if alt.upper() in token_dict  and token_dict[alt.upper()] > most_common_freq:\n",
    "                most_common_freq = token_dict[alt.upper()]\n",
    "                most_common = alt.upper()\n",
    "        new_sequence.append(most_common)\n",
    "        if (most_common != token):\n",
    "            print(most_common)\n",
    "            print(token)\n",
    "    \n",
    "    new_sentence = \" \".join(new_sentence)\n",
    "    new_sequence = \" \".join(new_sequence)\n",
    "        \n",
    "    mapped_lines.append(f'{new_sentence}\\t{new_sequence}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"elan_cleaned_synonyms.txt\"\n",
    "output = open(output_filename, \"w+\")\n",
    "\n",
    "for line in mapped_lines:\n",
    "    if \"indecipherable\" in line:\n",
    "        continue\n",
    "    if \"INDETERMINATE\" in line:\n",
    "        continue\n",
    "    output.write(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b321a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_filename = \"synonyms.txt\"\n",
    "# output = open(synonym_filename, \"w+\")\n",
    "\n",
    "# for token, freq in sorted_token_dict:\n",
    "#     output.write(f'{token}\\n')\n",
    "#     output.write(f'{str(synonyms[token])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2493a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(synonym_filename,\"r\")\n",
    "\n",
    "lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ae00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {}\n",
    "i = 0\n",
    "while i < len(lines):\n",
    "    \n",
    "    token = lines[i].strip()\n",
    "    i += 1\n",
    "    \n",
    "    alternatives = lines[i].strip()\n",
    "        \n",
    "    alternatives = alternatives[2:len(alternatives) - 2]\n",
    "    \n",
    "    alternatives = alternatives.split(\"', '\")\n",
    "    \n",
    "    synonyms[token] = alternatives\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in synonyms:\n",
    "    print(synonyms[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1745e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"elan_cleaned_synonyms.txt\",\"r\")\n",
    "\n",
    "lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_reference = []\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "    sentence = parts[0]\n",
    "    sequence = parts[1]\n",
    "    \n",
    "    sentence = sentence.replace(\"watch \", \"look \")\n",
    "    sentence = sentence.replace(\" watch\", \" look\")\n",
    "\n",
    "    sentence = sentence.replace(\"turtle \", \"tortoise \")\n",
    "    sentence = sentence.replace(\" turtle\", \" tortoise\")\n",
    "\n",
    "    sentence = sentence.replace(\"hare \", \"rabbit \")\n",
    "    sentence = sentence.replace(\" hare\", \" rabbit\")\n",
    "\n",
    "    sequence = sequence.replace(\"TORTOISE TORTOISE\", \"TORTOISE\")\n",
    "    sequence = sequence.replace(\"RABBIT RABBIT\", \"RABBIT\")\n",
    "\n",
    "            \n",
    "    converted_reference.append(f'{sentence}\\t{sequence}')\n",
    "    print(f'{sentence}\\t{sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"elan_cleaned_synonyms_converted.txt\"\n",
    "output = open(output_filename, \"w+\")\n",
    "\n",
    "for line in converted_reference:\n",
    "    output.write(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82778c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"elan_cleaned_merged.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b47337",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename, \"r\")\n",
    "lines = file.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc457615",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dict = {}\n",
    "token_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentence_dict = sorted(sentence_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_token_dict = sorted(token_dict.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_dict = {}\n",
    "token_dict = {}\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "        \n",
    "    for word in parts[0].split(\" \"):\n",
    "        word = word.strip('.,!:;\"').lower()\n",
    "        if word in sentence_dict:\n",
    "            sentence_dict[word] += 1\n",
    "        else:\n",
    "            sentence_dict[word] = 1\n",
    "        \n",
    "    for token in parts[1].split(\" \"):\n",
    "        token = token.strip('.,!:;\"\\n').upper()\n",
    "        if token in token_dict:\n",
    "            token_dict[token] += 1\n",
    "        else:\n",
    "            token_dict[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df28f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_lines = []\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "    sentence = parts[0]\n",
    "    sequence = parts[1]\n",
    "    \n",
    "    new_sentence = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        word = word.strip('\\'.,!:;\"\\n').lower()\n",
    "        new_sentence.append(word)\n",
    "        \n",
    "    new_sequence = []\n",
    "    for token in sequence.split(\" \"):\n",
    "        token = token.strip('\\'.,!:;\"\\n').upper()\n",
    "        if token == \"INDETERMINATE\":\n",
    "            continue\n",
    "        most_common_freq = token_dict[token]\n",
    "        most_common = token\n",
    "        if token in synonyms:\n",
    "            for alt in synonyms[token]:\n",
    "                if alt.upper() in token_dict and token_dict[alt.upper()] > most_common_freq:\n",
    "                    most_common_freq = token_dict[alt.upper()]\n",
    "                    most_common = alt.upper()\n",
    "        new_sequence.append(most_common)\n",
    "        if (most_common != token):\n",
    "            print(most_common)\n",
    "            print(token)\n",
    "    \n",
    "    new_sentence = \" \".join(new_sentence)\n",
    "    new_sequence = \" \".join(new_sequence)\n",
    "        \n",
    "    mapped_lines.append(f'{new_sentence}\\t{new_sequence}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899ff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(string):\n",
    "    return 1 + string.count(' ')\n",
    "\n",
    "converted_reference = []\n",
    "\n",
    "for line in mapped_lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "    sentence = parts[0]\n",
    "    sequence = parts[1]\n",
    "    \n",
    "    if \"xxx\" in sentence:\n",
    "        continue\n",
    "    \n",
    "#     if count_words(sequence) > count_words(sentence):\n",
    "#         continue\n",
    "    \n",
    "    \n",
    "    sentence = sentence.replace(\"watch \", \"look \")\n",
    "    sentence = sentence.replace(\" watch\", \" look\")\n",
    "\n",
    "    sentence = sentence.replace(\"turtle \", \"tortoise \")\n",
    "    sentence = sentence.replace(\" turtle\", \" tortoise\")\n",
    "\n",
    "    sentence = sentence.replace(\"hare \", \"rabbit \")\n",
    "    sentence = sentence.replace(\" hare\", \" rabbit\")\n",
    "\n",
    "    sequence = sequence.replace(\"TORTOISE TORTOISE\", \"TORTOISE\")\n",
    "    sequence = sequence.replace(\"RABBIT RABBIT\", \"RABBIT\")\n",
    "\n",
    "            \n",
    "    converted_reference.append(f'{sentence}\\t{sequence}')\n",
    "    print(f'{sentence}\\t{sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea719034",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"elan_cleaned_merged_synonyms.txt\"\n",
    "output = open(output_filename, \"w+\")\n",
    "\n",
    "for line in converted_reference:\n",
    "    output.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5091745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c331f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c1c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9d774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45ef3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb177b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f81bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
