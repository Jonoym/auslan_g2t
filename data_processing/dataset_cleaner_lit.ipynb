{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83efba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"elan_dataset_merged.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55faae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"elan_cleaned_merged.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7baba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(token):\n",
    "    \n",
    "    if token[:8] == \"FBUOY:DS\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 2:\n",
    "            token = token.split(':')[2]\n",
    "    if token[:6] == \"FBUOY:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    if token[:6] == \"FUBOY:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    if token[:6] == \"FBOUY:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    if token[:6] == \"GICA):\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    if token[:6] == \"TBUOY:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:3] == \"FS:\":\n",
    "        token = token[3:]\n",
    "    elif token[:3] == \"FB:\":\n",
    "        token = token[3:]\n",
    "    elif token[:2] == \"DS\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:2] == \"G(\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:3] == \"CA:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:3] == \"GA:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:2] == \"G:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:2] == \"M:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:2] == \"M:\":\n",
    "        split = token.split(':')\n",
    "        if len(split) > 1:\n",
    "            token = token.split(':')[1]\n",
    "    elif token[:7] == \"FINISH.\":\n",
    "        token = \"FINISH\"\n",
    "\n",
    "    index = token.find(\"-2H\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "    index = token.find(\"-1H\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "    index = token.find(\"2-H\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "    index = token.find(\"1-H\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "\n",
    "    index = token.find(\"(\")\n",
    "    if index != -1:\n",
    "        token = token[:index]\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    \n",
    "    sentence = sentence.replace(\"'s\", \" is\")\n",
    "    sentence = sentence.replace(\"n't\", \" not\")\n",
    "    sentence = sentence.replace(\"'d\", \" had\")\n",
    "    sentence = sentence.replace(\"'ll\", \" will\")\n",
    "    sentence = sentence.replace(\"'m\", \" am\")\n",
    "    sentence = sentence.replace(\"'ve\", \" have\")\n",
    "    sentence = sentence.replace(\"'re\", \" are\")\n",
    "    sentence = sentence.replace(\"ahh\", \"\")\n",
    "    sentence = sentence.replace(\"umm\", \"\")\n",
    "    sentence = sentence.replace(\"?\", \" ?\")\n",
    "\n",
    "    cleaned_sentence = \"\"\n",
    "    \n",
    "    for char in sentence:\n",
    "        if char.isalpha() or char == \"'\" or char == \"?\":\n",
    "            cleaned_sentence += char\n",
    "        else:\n",
    "            cleaned_sentence += \" \"\n",
    "    \n",
    "    cleaned_sentence = cleaned_sentence.lower()\n",
    "    cleaned_sentence = re.sub(' +', ' ', cleaned_sentence)\n",
    "    \n",
    "    return cleaned_sentence.strip()\n",
    "\n",
    "def should_skip_token(token):\n",
    "    if token[:3] == \"PT:\":\n",
    "        return True\n",
    "    if token[:5] == \"LOOK(\":\n",
    "        return True\n",
    "    if token[:7] == \"PTBUOY:\":\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "    \n",
    "def clean_tokens(token_sequence):\n",
    "    \n",
    "    token_sequence = token_sequence.replace(\"FALSE-START\", \"\")\n",
    "    token_sequence = token_sequence.replace(\"FALSE START\", \"\")\n",
    "    token_sequence = token_sequence.replace(\"?\", \" ?\")\n",
    "    tokens = token_sequence.strip().split(\" \")\n",
    "    \n",
    "    \n",
    "    cleaned_tokens = []\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token[:3] == \"PT:\":\n",
    "            continue\n",
    "        if token[:5] == \"LOOK(\":\n",
    "            continue\n",
    "        if token[:7] == \"PTBUOY:\":\n",
    "            continue\n",
    "        \n",
    "        token = get_token(token)\n",
    "        \n",
    "        if token in [\"WELL\", \"\", \"AHH\", \"UMM\", \"FSL\", \"PTBUOY\", \"HMM\", \"ERR\", \"PHOOEY\", \"INDETERMINATE\", \"INDECIPHERABLE\"]:\n",
    "            continue\n",
    "\n",
    "        if token[-1].isdigit():\n",
    "            token = token[:-1]\n",
    "\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "                        \n",
    "        cleaned_tokens.append(token)\n",
    "        \n",
    "    unduped_tokens = []\n",
    "    \n",
    "    for i in range(0, len(cleaned_tokens)):\n",
    "        if (i == 0 or cleaned_tokens[i] != cleaned_tokens[i - 1]) and (i < 2 or cleaned_tokens[i] != cleaned_tokens[i - 2]):\n",
    "            unduped_tokens.append(cleaned_tokens[i].replace(\"-\", \" \"))\n",
    "        \n",
    "    return (\" \".join(unduped_tokens)).upper()\n",
    "\n",
    "def spell_check(sequence, upper):\n",
    "    sequence = sequence.split(\" \")\n",
    "    output = []\n",
    "    for word in sequence:\n",
    "        if upper:\n",
    "            if spell.correction(word):\n",
    "                output.append(spell.correction(word).upper())\n",
    "            else:\n",
    "                output.append(word)\n",
    "        else:\n",
    "            if spell.correction(word):\n",
    "                output.append(spell.correction(word))\n",
    "            else:\n",
    "                output.append(word)\n",
    "    return \" \".join(output)\n",
    "\n",
    "def create_output_file(input_filename, output_filename):\n",
    "    input_file = open(input_filename, \"r\")\n",
    "    output_file = open(output_filename, \"w+\")\n",
    "    lines = input_file.readlines()\n",
    "    \n",
    "    cleaned_lines = []\n",
    "        \n",
    "    for line in lines:\n",
    "        split = line.split(\"\\t\")\n",
    "        \n",
    "        sentence = clean_sentence(clean_sentence(split[0]))\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "        sentence = spell_check(sentence, False)\n",
    "        \n",
    "        tokens = clean_tokens(clean_tokens(split[1]))\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        tokens = spell_check(tokens, True)\n",
    "        \n",
    "        if (len(sentence) == 0 or len(tokens) == 0) or sentence.count(\" \") > 25 or tokens.count(\" \") > 25:\n",
    "            continue\n",
    "        \n",
    "        cleaned_lines.append(f'{clean_sentence(split[0])}\\t{clean_tokens(split[1])}\\n')\n",
    "        \n",
    "    cleaned_lines.sort(key= lambda line: (line.split(\"\\t\")[0].count(\" \")))\n",
    "    \n",
    "    text = \"\"\n",
    "    \n",
    "    for cleaned_line in cleaned_lines:\n",
    "        text += cleaned_line\n",
    "        \n",
    "    output_file.write(text)\n",
    "    output_file.close()\n",
    "    input_file.close()\n",
    "    \n",
    "    return cleaned_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b20803",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = create_output_file(filename, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a499f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1ff9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ef7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37294cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e008f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c066398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc37d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111923f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b751e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a4f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1102fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98c4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac33150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a7071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe30791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac4a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_hist(sentences, index, bins, title, ylabel, xlabel):\n",
    "#     sentence_lengths = []\n",
    "\n",
    "#     for line in sentences:\n",
    "#         sentence = line.split(\"\\t\")\n",
    "#         count = sentence[index].count(\" \")\n",
    "#         sentence_lengths.append(count + 1)\n",
    "    \n",
    "#     plt.figure(figsize=(14,7)) # Make it 14x7 inch\n",
    "#     plt.style.use('seaborn-whitegrid') # nice and clean grid\n",
    "\n",
    "#     n, bins, patches = plt.hist(sentence_lengths, bins=bins, facecolor='#2ab0ff', edgecolor='#e0e0e0', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "#     n = n.astype('int') # it MUST be integer\n",
    "\n",
    "#     # Good old loop. Choose colormap of your taste\n",
    "#     for i in range(len(patches)):\n",
    "#         patches[i].set_facecolor(plt.cm.viridis(n[i]/max(n)))\n",
    "\n",
    "\n",
    "#     plt.xlabel(xlabel)\n",
    "#     plt.ylabel(ylabel)\n",
    "#     plt.title(title)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b32071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(sentences, 0, 25, \"\", \"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed86a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7024e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffa876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e2972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb3078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
