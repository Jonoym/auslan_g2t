{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pympi\n",
    "import re\n",
    "\n",
    "import random\n",
    "random.seed(20)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'EAF'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Datasets and Helper Functions\n",
    "\n",
    "Helper functions that are used in multiple sections of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(dataset_label, dataset, n=5):\n",
    "\n",
    "    print(f'Dataset: {dataset_label}\\n')\n",
    "\n",
    "    for i in range(0, n):\n",
    "        parts = dataset[i].split('\\t')\n",
    "        sentence = parts[0].strip()\n",
    "        sequence = parts[1].strip() \n",
    "        print(f'Sign Glosses:   {sequence}')\n",
    "        print(f'English Text:   {sentence}\\n')\n",
    "\n",
    "def join(glosses):\n",
    "    return \" \".join(glosses)\n",
    "\n",
    "def split_line(line):\n",
    "    parts = line.split('\\t')\n",
    "    return parts[0].strip(), parts[1].strip()\n",
    "\n",
    "def count_words(sentence):\n",
    "    return sentence.count(\" \") + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Data from the XML ELAN Files\n",
    "\n",
    "The settings included are the directory that contains the EAF files and the type of data that should be parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory, data_type):\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if '.eaf' in filename:\n",
    "            parse_file(directory, filename, dataset, data_type)\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def parse_file(directory, filename, dataset, data_type):\n",
    "    eaf = pympi.Elan.Eaf(f'./{directory}/{filename}')\n",
    "\n",
    "    sentences, sequences = parse_data(eaf, data_type)\n",
    "\n",
    "    if eaf.tiers.get(data_type) is None:\n",
    "        return\n",
    "\n",
    "    for i in range(0, len(sentences)):\n",
    "        if len(sentences[i]) > 40 or len(sequences[i]) > 30:\n",
    "            continue\n",
    "        if abs(len(sentences[i]) - len(sequences[i])) > 30:\n",
    "            continue \n",
    "        dataset.append(f'{sentences[i]}\\t{join(sequences[i])}\\n')\n",
    "\n",
    "def parse_data(eaf, data_type):\n",
    "    sentences = filter_tier(eaf, data_type)\n",
    "\n",
    "    left_gloss = filter_tier(eaf, 'LH-IDgloss')\n",
    "    right_gloss = filter_tier(eaf, 'RH-IDgloss')\n",
    "\n",
    "    glosses = parse_glosses(sentences, left_gloss, right_gloss)\n",
    "\n",
    "    return filter_sentences(sentences), filter_glosses(glosses)\n",
    "\n",
    "\n",
    "def parse_glosses(sentences, left_gloss, right_gloss):\n",
    "    lhp = 0\n",
    "    rhp = 0\n",
    "\n",
    "    glosses = []\n",
    "    for sentence in sentences:\n",
    "        gloss_group = []\n",
    "        while lhp < len(left_gloss) and left_gloss[lhp][1] <= sentence[1] + 2:\n",
    "            gloss_group.append(left_gloss[lhp])\n",
    "            lhp += 1\n",
    "        while rhp < len(right_gloss) and right_gloss[rhp][1] <= sentence[1] + 2:\n",
    "            gloss_group.append(right_gloss[rhp])\n",
    "            rhp += 1\n",
    "        glosses.append(gloss_group)\n",
    "    return glosses\n",
    "    \n",
    "def filter_sentences(sentences):\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        filtered_sentences.append(sentence[2])\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "def filter_glosses(glosses):\n",
    "    filtered_glosses = []\n",
    "\n",
    "    for gloss_group in glosses:\n",
    "        gloss_group.sort()\n",
    "        filtered_gloss_group = []\n",
    "        for i in range(0, len(gloss_group)):\n",
    "            if i == 0 or gloss_group[i][2] != gloss_group[i - 1][2]:\n",
    "                filtered_gloss_group.append(gloss_group[i][2])\n",
    "        filtered_glosses.append(filtered_gloss_group)\n",
    "\n",
    "    return filtered_glosses\n",
    "\n",
    "def filter_tier(eaf, data_type):\n",
    "    segment_dict = eaf.tiers[data_type][0]\n",
    "    \n",
    "    filtered_segments = []\n",
    "    \n",
    "    for key in segment_dict:\n",
    "        segment = segment_dict[key]\n",
    "        if segment[2] != '':\n",
    "            filtered_segments.append((int(segment[0][2:]), int(segment[1][2:]), segment[2]))\n",
    "        \n",
    "    return filtered_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Splitting Sign Decorators for Gloss Sequences and Sentences\n",
    "\n",
    "Removing the sign decorators or splitting them into their own tokens. Sentences will also be processed to be more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset, remove=True):\n",
    "\n",
    "    filtered_dataset = []\n",
    "\n",
    "    for line in dataset:\n",
    "        sentence, sequence = split_line(line)\n",
    "\n",
    "        sentence = clean_sentence(sentence)\n",
    "\n",
    "        sequence = clean_sequence(sequence, remove)\n",
    "\n",
    "        if remove:\n",
    "            sequence = clean_sequence_removal(sequence)\n",
    "\n",
    "        if len(sentence) == 0 or len(sequence) == 0:\n",
    "            continue\n",
    "\n",
    "        if count_words(sentence) > 25 or count_words(sequence) > 25:\n",
    "            continue\n",
    "\n",
    "        if abs(count_words(sentence) - count_words(sequence)) > 7:\n",
    "            continue\n",
    "    \n",
    "        filtered_dataset.append(f'{sentence}\\t{sequence}\\n')\n",
    "\n",
    "    return filtered_dataset\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "\n",
    "    sentence = sentence.replace(\"'s\", \" is\")\n",
    "    sentence = sentence.replace(\"n't\", \" not\")\n",
    "    sentence = sentence.replace(\"'d\", \" had\")\n",
    "    sentence = sentence.replace(\"'ll\", \" will\")\n",
    "    sentence = sentence.replace(\"'m\", \" am\")\n",
    "    sentence = sentence.replace(\"'ve\", \" have\")\n",
    "    sentence = sentence.replace(\"'re\", \" are\")\n",
    "    sentence = sentence.replace(\"ahh\", \"\")\n",
    "    sentence = sentence.replace(\"umm\", \"\")\n",
    "    sentence = sentence.replace(\"?\", \" ?\")\n",
    "\n",
    "    cleaned_sentence = \"\"\n",
    "    \n",
    "    for char in sentence:\n",
    "        if char.isalpha() or char == \"'\" or char == \"?\":\n",
    "            cleaned_sentence += char\n",
    "        else:\n",
    "            cleaned_sentence += \" \"\n",
    "    \n",
    "    cleaned_sentence = cleaned_sentence.lower()\n",
    "    cleaned_sentence = re.sub(' +', ' ', cleaned_sentence)\n",
    "    \n",
    "    return cleaned_sentence.strip()\n",
    "\n",
    "def clean_sequence(sequence, remove):\n",
    "    sequence = sequence.replace(\"FALSE-START\", \"\")\n",
    "    sequence = sequence.replace(\"FALSE START\", \"\")\n",
    "    sequence = sequence.replace(\"?\", \" ?\")\n",
    "\n",
    "    glosses = sequence.strip().replace(\"-\",\" \").replace(\"):\",\" \").split(\" \")\n",
    "    cleaned_glosses = []\n",
    "        \n",
    "    for gloss in glosses:\n",
    "        if gloss[:3] == \"PT:\" or gloss[:5] == \"LOOK(\" or gloss[:7] == \"PTBUOY:\":\n",
    "            continue\n",
    "        \n",
    "        gloss = get_gloss(gloss, remove)\n",
    "        \n",
    "        if gloss in [\"THE\", \"IN\", \"WELL\", \"\", \"AHH\", \"UMM\", \"FSL\", \"PTBUOY\", \"HMM\", \"ERR\", \"PHOOEY\", \"INDETERMINATE\", \"INDECIPHERABLE\"]:\n",
    "            continue\n",
    "\n",
    "        if gloss[-1].isdigit():\n",
    "            gloss = gloss[:-1]\n",
    "\n",
    "        if len(gloss) < 2:\n",
    "            continue\n",
    "                        \n",
    "        cleaned_glosses.append(gloss)\n",
    "\n",
    "    unduped_glosses = []\n",
    "    \n",
    "    for i in range(0, len(cleaned_glosses)):\n",
    "        if (i == 0 or cleaned_glosses[i] != cleaned_glosses[i - 1]) and (i < 2 or cleaned_glosses[i] != cleaned_glosses[i - 2]):\n",
    "            unduped_glosses.append(cleaned_glosses[i].replace(\"-\", \" \"))\n",
    "        \n",
    "    return (\" \".join(unduped_glosses)).upper()\n",
    "\n",
    "def clean_sequence_removal(sequence):\n",
    "    \n",
    "    glosses = sequence.split(\" \")\n",
    "\n",
    "    filtered_glosses = []\n",
    "\n",
    "    for gloss in glosses:\n",
    "        if gloss in ['THE', 'DS', 'DSM', 'DSL']:\n",
    "            continue\n",
    "        filtered_glosses.append(gloss)\n",
    "    \n",
    "    return (\" \".join(filtered_glosses)).upper()\n",
    "            \n",
    "\n",
    "def get_gloss(gloss, remove):\n",
    "    \n",
    "    removed_decorator_start = \"\"\n",
    "    removed_decorator_end = \"\"\n",
    "\n",
    "    if gloss[:8] == \"FBUOY:DS\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 2:\n",
    "            gloss = gloss.split(':')[2]\n",
    "            removed_decorator_start = \"FBUOY \"\n",
    "    elif gloss[:6] == \"FBUOY:\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "            removed_decorator_start = \"FBUOY \"\n",
    "    elif gloss[:6] == \"FUBOY:\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "            removed_decorator_start = \"FBUOY \"\n",
    "    elif gloss[:6] == \"FBOUY:\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "            removed_decorator_start = \"FBUOY \"\n",
    "    elif gloss[:6] == \"GICA):\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "    elif gloss[:6] == \"TBUOY:\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "            removed_decorator_start = \"FBUOY \"\n",
    "    elif gloss[:3] == \"FS:\":\n",
    "        gloss = gloss[3:]\n",
    "        removed_decorator_start = \"FS \"\n",
    "    elif gloss[:3] == \"FB:\":\n",
    "        gloss = gloss[3:]\n",
    "        removed_decorator_start = \"FB \"\n",
    "    elif gloss[:2] == \"DS\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "            removed_decorator_start = \"DS \"\n",
    "    elif gloss[:2] == \"G(\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "    elif gloss[:3] == \"CA:\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "    elif gloss[:3] == \"GA:\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "    elif gloss[:2] == \"G:\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "    elif gloss[:2] == \"M:\":\n",
    "        split = gloss.split(':')\n",
    "        if len(split) > 1:\n",
    "            gloss = gloss.split(':')[1]\n",
    "    elif gloss[:7] == \"FINISH.\":\n",
    "        gloss = \"FINISH\"\n",
    "\n",
    "    index = gloss.find(\"-2H\")\n",
    "    if index != -1:\n",
    "        gloss = gloss[:index]\n",
    "        removed_decorator_start = \" 2H\"\n",
    "    index = gloss.find(\"-1H\")\n",
    "    if index != -1:\n",
    "        gloss = gloss[:index]\n",
    "        removed_decorator_start = \" 1H\"\n",
    "    index = gloss.find(\"2-H\")\n",
    "    if index != -1:\n",
    "        gloss = gloss[:index]\n",
    "        removed_decorator_start = \" 2H\"\n",
    "    index = gloss.find(\"1-H\")\n",
    "    if index != -1:\n",
    "        gloss = gloss[:index]\n",
    "        removed_decorator_start = \" 1H\"\n",
    "\n",
    "    index = gloss.find(\"(\")\n",
    "    if index != -1:\n",
    "        gloss = gloss[:index]\n",
    "    \n",
    "    if remove:\n",
    "        return gloss\n",
    "    return f'{removed_decorator_start}{gloss}{removed_decorator_end}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalisation of Glosses with Synonyms\n",
    "\n",
    "Different sign glosses have been used for signs that represent the same thing. This is an error with the dataset which has been attempted to be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms():\n",
    "    synonym_filename = 'synonyms.txt'\n",
    "\n",
    "    file = open(synonym_filename, 'r')\n",
    "    lines = file.readlines()\n",
    "\n",
    "    synonyms = {}\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        gloss = lines[i].strip()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        alternatives = lines[i].strip()\n",
    "        alternatives = alternatives[2:len(alternatives) - 2]\n",
    "        alternatives = alternatives.split(\"', '\")\n",
    "\n",
    "        synonyms[gloss] = alternatives\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return synonyms\n",
    "\n",
    "def count_frequencies(dataset):\n",
    "\n",
    "    gloss_dict = {}\n",
    "\n",
    "    for line in dataset:\n",
    "        parts = line.split(\"\\t\")\n",
    "            \n",
    "        for gloss in parts[1].split(\" \"):\n",
    "            gloss = gloss.strip('.,!:;\"\\n').upper()\n",
    "            if gloss in gloss_dict:\n",
    "                gloss_dict[gloss] += 1\n",
    "            else:\n",
    "                gloss_dict[gloss] = 1\n",
    "\n",
    "    return gloss_dict\n",
    "\n",
    "def clean_sentence_duplicates(sentence):\n",
    "    sentence = sentence.replace(\"watch \", \"look \")\n",
    "    sentence = sentence.replace(\" watch\", \" look\")\n",
    "\n",
    "    sentence = sentence.replace(\"turtle \", \"tortoise \")\n",
    "    sentence = sentence.replace(\" turtle\", \" tortoise\")\n",
    "\n",
    "    sentence = sentence.replace(\"hare \", \"rabbit \")\n",
    "    sentence = sentence.replace(\" hare\", \" rabbit\")\n",
    "    return sentence\n",
    "\n",
    "def clean_sequence_duplicates(sequence):\n",
    "    sequence = sequence.replace(\"TORTOISE TORTOISE\", \"TORTOISE\")\n",
    "    sequence = sequence.replace(\"RABBIT RABBIT\", \"RABBIT\")\n",
    "    return sequence\n",
    "\n",
    "def generalise_synonyms(dataset):\n",
    "\n",
    "    synonyms = get_synonyms()\n",
    "\n",
    "    gloss_dict = count_frequencies(dataset)\n",
    "\n",
    "    converted_dataset = []\n",
    "\n",
    "    for line in dataset:\n",
    "        sentence, sequence = split_line(line)\n",
    "\n",
    "        converted_sequence = []\n",
    "\n",
    "        for gloss in sequence.split(\" \"):\n",
    "            if gloss == \"INDETERMINATE\":\n",
    "                continue\n",
    "        \n",
    "            most_common_freq = gloss_dict[gloss]\n",
    "            most_common = gloss\n",
    "\n",
    "            if gloss in synonyms:\n",
    "                for alt in synonyms[gloss]:\n",
    "                    if alt.upper() in gloss_dict and gloss_dict[alt.upper()] > most_common_freq:\n",
    "                        most_common_freq = gloss_dict[alt.upper()]\n",
    "                        most_common = alt.upper()\n",
    "            \n",
    "            converted_sequence.append(most_common)\n",
    "\n",
    "        sentence = clean_sentence_duplicates(sentence)\n",
    "        sequence = clean_sequence_duplicates(sequence)\n",
    "        \n",
    "        converted_dataset.append(f'{sentence}\\t{join(converted_sequence)}')\n",
    "    \n",
    "    return converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "dataset = []\n",
    "generalise_synonyms(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Dataset Parsing and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Raw Free Translation\n",
      "\n",
      "Sign Glosses:   G:ROLLS DOG1 G:OH\n",
      "English Text:   The dog was a bit stunned.\n",
      "\n",
      "Sign Glosses:   WITH1 DOG1 DS(1):THE-DEER-RUNS DS(1):ANIMAL-RUNS G(HOLD-PAUSE):UMM\n",
      "English Text:   The dog ran along beside them.\n",
      "\n",
      "Sign Glosses:   CA:TURTLE SHOW1-2H\n",
      "English Text:   \"Ahh...see, see\".\n",
      "\n",
      "Sign Glosses:   PASS FINISH.GOOD-2H G(CA):\n",
      "English Text:   \"Has he already passed me?\"\n",
      "\n",
      "Sign Glosses:   STORY BOY PT:LOC\n",
      "English Text:   This story is about the boy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "free_dataset = create_dataset(directory, 'FreeTransl')\n",
    "display('Raw Free Translation', free_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Raw Free Translation\n",
      "\n",
      "Sign Glosses:   ROLLS DOG OH\n",
      "English Text:   the dog was a bit stunned\n",
      "\n",
      "Sign Glosses:   WITH DOG DEER RUNS ANIMAL RUNS PAUSE\n",
      "English Text:   the dog ran along beside them\n",
      "\n",
      "Sign Glosses:   TURTLE SHOW 2H\n",
      "English Text:   ahh see see\n",
      "\n",
      "Sign Glosses:   PASS FINISH 2H\n",
      "English Text:   has he already passed me ?\n",
      "\n",
      "Sign Glosses:   STORY BOY\n",
      "English Text:   this story is about the boy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removed_free_dataset = filter_dataset(free_dataset, remove=True)\n",
    "display('Raw Free Translation', removed_free_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Raw Free Translation\n",
      "\n",
      "Sign Glosses:   ROLLS DOG OH\n",
      "English Text:   the dog was a bit stunned\n",
      "\n",
      "Sign Glosses:   WITH DOG DS DEER RUNS DS ANIMAL RUNS PAUSE\n",
      "English Text:   the dog ran along beside them\n",
      "\n",
      "Sign Glosses:   TURTLE SHOW 2H\n",
      "English Text:   ahh see see\n",
      "\n",
      "Sign Glosses:   PASS FINISH 2H\n",
      "English Text:   has he already passed me ?\n",
      "\n",
      "Sign Glosses:   STORY BOY\n",
      "English Text:   this story is about the boy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retained_free_dataset = filter_dataset(free_dataset, remove=False)\n",
    "display('Raw Free Translation', retained_free_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literal Dataset Parsing and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Raw Literal Translation\n",
      "\n",
      "Sign Glosses:   RIGHT THINK\n",
      "English Text:   (He) thought\n",
      "\n",
      "Sign Glosses:   LOOK SMELL1 PUSH TREE2 PUSH COINCIDENCE DSM(BC):SPHERICAL-BEEHIVE-FALLS DSG(BENT5):VERTICAL-TREE-TRUNK? SPILL\n",
      "English Text:   Then, (the hive) fell breaking.\n",
      "\n",
      "Sign Glosses:   DS(B):RABBIT-RUNNING\n",
      "English Text:   (He) scampered.\n",
      "\n",
      "Sign Glosses:   BIRD PT:LOC FLY1 PT: G(CA): WALK-1H\n",
      "English Text:   (He) walked.\n",
      "\n",
      "Sign Glosses:   G(CA):HUMAN-RESTS-HIS-HAND-ON-THE-BRANCH LOOK WHERE-1H FROG G(CA):HUMAN-RESTS-HIS-HAND-ON-THE-BRANCH SHOCK SURPRISED PT:PRO3SG MOVE\n",
      "English Text:   It moved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lit_dataset = create_dataset(directory, 'LitTransl')\n",
    "display('Raw Literal Translation', lit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Removed Literal Translation\n",
      "\n",
      "Sign Glosses:   RIGHT THINK\n",
      "English Text:   he thought\n",
      "\n",
      "Sign Glosses:   RABBIT RUNNING\n",
      "English Text:   he scampered\n",
      "\n",
      "Sign Glosses:   BIRD FLY WALK 1H\n",
      "English Text:   he walked\n",
      "\n",
      "Sign Glosses:   DSS SPHERICAL JAR ANIMAL WALKS BACKWARD\n",
      "English Text:   the frog walks back\n",
      "\n",
      "Sign Glosses:   FETCH TEN BOWLING BALL\n",
      "English Text:   he gets a ten pin bowling ball\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removed_lit_dataset = filter_dataset(lit_dataset, remove=True)\n",
    "display('Removed Literal Translation', removed_lit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Generalised Literal Translation\n",
      "\n",
      "Sign Glosses:   RIGHT THINK\n",
      "English Text:   he thought\n",
      "\n",
      "Sign Glosses:   RABBIT RUN\n",
      "English Text:   he scampered\n",
      "\n",
      "Sign Glosses:   BIRD FLY WALK 1H\n",
      "English Text:   he walked\n",
      "\n",
      "Sign Glosses:   DSS SPHERICAL JAR ANIMAL WALKS BACKWARD\n",
      "English Text:   the frog walks back\n",
      "\n",
      "Sign Glosses:   TAKE TEN BOWLING BALL\n",
      "English Text:   he gets a ten pin bowling ball\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generalised_lit_dataset = generalise_synonyms(removed_lit_dataset)\n",
    "display('Generalised Literal Translation', generalised_lit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
