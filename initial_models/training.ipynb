{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87606bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3864f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, Add, Activation, Embedding, Reshape\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67735c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename before the train/test and filetype\n",
    "filename = \"phoenix\"\n",
    "\n",
    "dataset = load_clean_sentences(filename + '-both.pkl')\n",
    "train = load_clean_sentences(filename + '-train.pkl')\n",
    "test = load_clean_sentences(filename + '-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb52b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Functions\n",
    "\n",
    "# Constructs the Tokenizer for each language in the Translation\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# Determines the Longest Sentences contained in the Dataset\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "# Encodes and Pad word sequences to fill the maximum length\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# One-hot Encode the output sequences\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17072ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97186b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "y_train = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "y_train = encode_output(y_train, eng_vocab_size)\n",
    "\n",
    "x_test = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "y_test = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "y_test = encode_output(y_test, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMT Model\n",
    "\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 1000)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# fit model\n",
    "filename = 'model3.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(x_train, y_train, epochs=30, batch_size=1, validation_data=(x_test, y_test), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb9640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f827d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf0122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e79153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
